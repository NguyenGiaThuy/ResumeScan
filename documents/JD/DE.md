# 📌 Job Title: Data Engineer (Cloud | Multi-domain Projects)

## 📍 Location
Ho Chi Minh City, Vietnam (Onsite or Hybrid)

## 🕒 Employment Type
Full-time | Permanent

## 💼 Experience Level
Mid-Level (Minimum 3 years of relevant experience)

---

## 🏢 About the Company
We are a leading global IT solutions provider delivering high-impact data-driven services and digital transformation projects to clients across industries including **Banking, Healthcare, Manufacturing, Energy, and Retail**. As part of our expansion in Ho Chi Minh City, we are looking for talented **Data Engineers** to join our fast-growing team.

---

## 🎯 Responsibilities

- Build, optimize, and maintain **scalable data pipelines** for ingestion, transformation, and storage using **Apache Spark / PySpark**.
- Develop and manage **cloud-native data workflows** on platforms like **AWS (Glue, Lambda, Redshift)** or **Azure (ADF, Synapse)**.
- Design and implement **data lakes / lakehouses** using **Delta Lake**, **Apache Iceberg**, or **Apache Hudi**.
- Integrate data from various sources including **ERP**, **CRM**, **IoT**, and **external APIs** using **Kafka**, **NiFi**, or **Fluentd**.
- Collaborate with cross-functional teams to ensure **data governance**, **data quality**, and **privacy compliance**.
- Develop **SQL-based models and views** for analytics and BI tools (e.g., Power BI, Tableau, Superset).
- Implement **orchestration and monitoring** workflows using **Apache Airflow** or **Dagster**.
- Document data flows, schemas, and technical specifications.

---

## ✅ Requirements

### 🎓 Technical

- **Minimum 3 years** of experience as a Data Engineer or in a similar data role.
- Proficient in **Python** and **SQL**, especially for ETL logic and transformation.
- Strong experience in:
  - **Cloud platforms**:  
    - AWS (Glue, S3, Lambda, Redshift) **OR**  
    - Azure (ADF, Synapse, Data Lake Storage)
  - **Distributed data processing**: Spark / PySpark
  - **Data orchestration**: Airflow, Prefect, or Dagster
  - **Real-time data streaming**: Kafka, Kinesis, or Pub/Sub
  - **Table formats**: Delta Lake, Apache Iceberg, or Apache Hudi
  - **Storage systems**: S3, Azure Blob, HDFS, MinIO
- Familiarity with:
  - **CI/CD tools** (e.g., GitHub Actions, GitLab CI)
  - **Containerization** (Docker)
  - **Metadata and cataloging tools** (Apache Atlas, OpenMetadata)
  - **Data quality frameworks** (Great Expectations, soda-core)

### 🌍 English Communication

- **Strong English proficiency** – both written and spoken
- Able to **participate in customer meetings**, **clarify requirements**, and **document solutions in English**
- Previous experience in customer-facing or offshore/onsite communication roles is a plus

---

## 💡 Nice to Have

- Cloud certification: **AWS Certified Data Analytics**, **Azure Data Engineer Associate**, or **GCP Professional Data Engineer**
- Experience in **DataOps / DevOps** for data pipeline deployment
- Domain experience in **Finance**, **Healthcare**, **Manufacturing**, or **Retail analytics**

---

## 🌟 Why Join Us?

- Opportunity to work on **global projects** with top-tier clients
- Clear career path and **sponsorship for certifications**
- Modern, flexible, and international working environment
- Competitive salary + **13th month bonus** + performance incentives
- Access to **training programs**, tech seminars, and mentorship

---

## 📨 How to Apply

Send your CV (in English) to: **careers@yourcompany.com**  
Subject: **[Data Engineer - HCM] – Your Full Name**

